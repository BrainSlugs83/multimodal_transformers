{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y117dIimYVuW"
   },
   "source": [
    "This Colab generates language and image features to be used with pretrained image--language transformers.  It then allows you to use our released models to determine if an image-text pair match!\n",
    "\n",
    "This replicates our retrieval results in our TACL 2021 paper:\n",
    "\n",
    "[Decoupling the Role of Data, Attention, and Losses in Multimodal Transformers](https://arxiv.org/abs/2102.00529)\n",
    "\n",
    "Paper Authors:  Lisa Anne Hendricks, John Mellor, Rosalia Schneider, Jean-Baptiste Alayrac, and Aida Nematzadeh\n",
    "\n",
    "We also thank Sebastian Borgeaud and Cyprien de Masson d'Autume for their text preprocessing code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_oXEX6F6_Rv"
   },
   "source": [
    "# Preproccessing Language and Images\n",
    "\n",
    "First, we use a detector to extract image features and SentencePiece to extract language tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "vmooLaXxzPS-"
   },
   "outputs": [],
   "source": [
    "# Make sure to follow the Setup\\Instructions.md steps first.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "import tensorflow_hub as hub\n",
    "from io import BytesIO as StringIO\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "kumubIq2g803"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\projects.py\\tf-models\\research\n"
     ]
    }
   ],
   "source": [
    "# Replace with the path where you setup the models repo, when you followed the Setup\\Instructions.md steps\n",
    "%cd ..\\tf-models\\research\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "cyCSxln6hYDj"
   },
   "outputs": [],
   "source": [
    "from object_detection.utils import visualization_utils as vis_util\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.core import standard_fields as fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "0OCoLQ6msv0X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to download\n",
      "    from: 'https://storage.googleapis.com/dm-mmt-models/spiece.model'\n",
      "      to: 'C:\\Users\\Mikey\\AppData\\Local\\Temp\\spiece.model'\n"
     ]
    }
   ],
   "source": [
    "!wget  https://storage.googleapis.com/dm-mmt-models/spiece.model -P '/tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3Q9VtFoEJeMY"
   },
   "outputs": [],
   "source": [
    "features = {} # input to our model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oiZXWgUln-nM"
   },
   "source": [
    "## Language Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6WQsSwvoCwv"
   },
   "source": [
    "### Helper Functions for Preprocessing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "z-l7ezeGlN3o"
   },
   "outputs": [],
   "source": [
    "SPIECE_UNDERLINE = '‚ñÅ'  # pylint: disable=invalid-encoded-data\n",
    "\n",
    "special_symbols = {\n",
    "    '<cls>': 3,\n",
    "    '<sep>': 4,\n",
    "    '<pad>': 5,\n",
    "    '<mask>': 6,\n",
    "}\n",
    "CLS_ID = special_symbols['<cls>']\n",
    "SEP_ID = special_symbols['<sep>']\n",
    "PAD_ID = special_symbols['<pad>']\n",
    "MASK_ID = special_symbols['<mask>']\n",
    "\n",
    "def is_start_piece(piece):\n",
    "  \"\"\"Returns True if the piece is a start piece for a word/symbol.\"\"\"\n",
    "  special_pieces = set(list('!\"#$%&\\\"()*+,-./:;?@[\\\\]^_`{|}~'))\n",
    "  if piece.startswith(SPIECE_UNDERLINE):\n",
    "    return True\n",
    "  if piece.startswith('<'):\n",
    "    return True\n",
    "  if piece in special_pieces:\n",
    "    return True\n",
    "  return False\n",
    "\n",
    "\n",
    "def preprocess_text(inputs, lower=False, remove_space=True, keep_accents=False):\n",
    "  \"\"\"Preprocess the inputs.\"\"\"\n",
    "  if remove_space:\n",
    "    outputs = ' '.join(inputs.strip().split())\n",
    "  else:\n",
    "    outputs = inputs\n",
    "  outputs = outputs.replace('``', '\"').replace('\\'\\'', '\"')\n",
    "\n",
    "  if not keep_accents:\n",
    "    outputs = unicodedata.normalize('NFKD', outputs)\n",
    "    outputs = ''.join([c for c in outputs if not unicodedata.combining(c)])\n",
    "  if lower:\n",
    "    outputs = outputs.lower()\n",
    "\n",
    "  return outputs\n",
    "\n",
    "\n",
    "def encode_pieces(sp_model, text, sample=False):\n",
    "  \"\"\"Encode the text to pieces using the given SentencePiece model sp_model.\"\"\"\n",
    "  if not sample:\n",
    "    pieces = sp_model.EncodeAsPieces(text)\n",
    "  else:\n",
    "    pieces = sp_model.SampleEncodeAsPieces(text, 64, 0.1)\n",
    "  new_pieces = []\n",
    "  for piece in pieces:\n",
    "    if len(piece) > 1 and piece[-1] == ',' and piece[-2].isdigit():\n",
    "      cur_pieces = sp_model.EncodeAsPieces(\n",
    "          piece[:-1].replace(SPIECE_UNDERLINE, ''))\n",
    "      if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0] == SPIECE_UNDERLINE:\n",
    "        if len(cur_pieces[0]) == 1:\n",
    "          cur_pieces = cur_pieces[1:]\n",
    "        else:\n",
    "          cur_pieces[0] = cur_pieces[0][1:]\n",
    "      cur_pieces.append(piece[-1])\n",
    "      new_pieces.extend(cur_pieces)\n",
    "    else:\n",
    "      new_pieces.append(piece)\n",
    "\n",
    "  return new_pieces\n",
    "\n",
    "\n",
    "def encode_ids(sp_model, text, sample=False):\n",
    "  pieces = encode_pieces(sp_model, text, sample=sample)\n",
    "  ids = [sp_model.PieceToId(piece) for piece in pieces]\n",
    "  return ids\n",
    "\n",
    "\n",
    "def tokens_to_word_indices(sp_model, tokens, offset=0):\n",
    "    \"\"\"Compute the word ids for the tokens.\n",
    "\n",
    "    The word indices start at offset, each time a new word is encountered, the\n",
    "    word id is increased by 1.\n",
    "\n",
    "    Args:\n",
    "      tokens: `list` of `int` SentencePiece tokens\n",
    "      offset: `int` start index\n",
    "\n",
    "    Returns:\n",
    "      A `list` of increasing integers. If element i and j are identical, then\n",
    "      tokens[i] and tokens[j] are part of the same word.\n",
    "    \"\"\"\n",
    "    word_indices = []\n",
    "    current_index = offset\n",
    "    for i, token in enumerate(tokens):\n",
    "      token_piece = sp_model.IdToPiece(token)\n",
    "      if i > 0 and is_start_piece(token_piece):\n",
    "        current_index += 1\n",
    "      word_indices.append(current_index)\n",
    "\n",
    "    return word_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "um98ejShoRNB"
   },
   "source": [
    "### Load the SentencePiece Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ENAioP-BoOgZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sentencepiece as sp\n",
    "spm_path = os.environ['temp'] + '\\spiece.model'\n",
    "spm = sp.SentencePieceProcessor()\n",
    "spm.Load(spm_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yOIiWM_WoP5K"
   },
   "source": [
    "### Preprocessing Captions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "fA-h8RzDKRqT"
   },
   "outputs": [],
   "source": [
    "def create_sentence_features(seq_len, spm, captions, max_sentence_number=1):\n",
    "  def _add_sentence_pad():\n",
    "    pad_number = max_sentence_number - len(captions)\n",
    "    for _ in range(pad_number):\n",
    "      all_sents['tokens'] += [MASK_ID] * seq_len\n",
    "      all_sents['segment_ids'] += [0] * seq_len\n",
    "      all_sents['padding_mask'] += [1] * seq_len\n",
    "      all_sents['word_ids'] += [-2] * seq_len\n",
    "\n",
    "  # Limit the sentence length to seq_len\n",
    "  # We concatenate all sentences after checking the seq len and adding\n",
    "  # padding\n",
    "  all_sents = {}\n",
    "  for k in ['tokens', 'segment_ids', 'padding_mask', 'word_ids']:\n",
    "    all_sents[k] = []\n",
    "\n",
    "  for sentence in captions:\n",
    "    sentence = preprocess_text(sentence, remove_space=True, lower=True, keep_accents=False)\n",
    "\n",
    "    tokens = encode_ids(spm, sentence)\n",
    "    if len(tokens) >= seq_len - 2:\n",
    "      tokens = tokens[:seq_len - 2]  # since we add two symbols\n",
    "\n",
    "    word_ids = tokens_to_word_indices(spm, tokens)\n",
    "    word_ids = ([-1] + word_ids + [-1])\n",
    "    # Need to create segment ids before adding special symbols to tokens\n",
    "    segment_ids = ([0] +  # SEP\n",
    "                    [0] * len(tokens) + [2]  # CLS\n",
    "                  )\n",
    "    tokens = ([SEP_ID] + tokens + [CLS_ID])\n",
    "    padding_mask = [0] * len(tokens)\n",
    "    # Note, we add padding at the start so that the last token is always [CLS]\n",
    "\n",
    "    if len(tokens) < seq_len:\n",
    "      padding_len = seq_len - len(tokens)\n",
    "      tokens = [MASK_ID] * padding_len + tokens\n",
    "      \n",
    "      segment_ids = [0] * padding_len + segment_ids\n",
    "      padding_mask = [1] * padding_len + padding_mask\n",
    "      word_ids = [-2] * padding_len + word_ids\n",
    "\n",
    "\n",
    "    assert len(tokens) == seq_len\n",
    "    assert len(segment_ids) == seq_len\n",
    "    assert len(padding_mask) == seq_len\n",
    "    assert len(word_ids) == seq_len\n",
    "\n",
    "    all_sents['tokens'] += tokens\n",
    "    all_sents['segment_ids'] += segment_ids\n",
    "    all_sents['padding_mask'] += padding_mask\n",
    "    all_sents['word_ids'] += word_ids\n",
    "\n",
    "  # Add padding sentences to the end so that each example has\n",
    "  # max_sentence_number\n",
    "  if len(captions) < max_sentence_number:\n",
    "    _add_sentence_pad()\n",
    "\n",
    "  return {\n",
    "      'text/token_ids': np.array(all_sents['tokens'], dtype=np.int32),\n",
    "      'text/segment_ids': np.array(all_sents['segment_ids'], dtype=np.int32),\n",
    "      'text/padding_mask': np.array(all_sents['padding_mask'], dtype=np.int32),\n",
    "      'text/word_ids': np.array(all_sents['word_ids'], dtype=np.int32),\n",
    "      'text/sentence_num': len(captions),\n",
    "  }\n",
    "         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dlV7HY-SLEj4"
   },
   "source": [
    "Get features for an example caption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "VXYlQ_rANxtZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text/token_ids': array([    6,     6,     6,     6,     6,     6,     6,     6,     6,\n",
      "           6,     6,     6,     6,     6,     4,    24,   326,    33,\n",
      "          24, 14559,  1757,    24, 22968,     9,     3]), 'text/segment_ids': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 2]), 'text/padding_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0]), 'text/word_ids': array([-2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -1,  0,  1,\n",
      "        2,  3,  4,  5,  6,  7,  8, -1]), 'text/sentence_num': 1}\n"
     ]
    }
   ],
   "source": [
    "features = create_sentence_features(seq_len=25, spm=spm, captions=['A man with a backpack holding a kitten.'])\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_uRSZXmodsj"
   },
   "source": [
    "## Image Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PczZB4Z4FvUt"
   },
   "source": [
    "###Load the Pretrained Object Detector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "-xSz_GEZFoH0"
   },
   "outputs": [],
   "source": [
    "def LoadInferenceGraph(inference_graph_path):\n",
    "  \"\"\"Loads inference graph into tensorflow Graph object.\n",
    "\n",
    "  Args:\n",
    "    inference_graph_path: Path to inference graph.\n",
    "\n",
    "  Returns:\n",
    "    a tf.Graph object.\n",
    "  \"\"\"\n",
    "  od_graph = tf.Graph()\n",
    "  with od_graph.as_default():\n",
    "    od_graph_def = tf.GraphDef()\n",
    "    with open(inference_graph_path, 'rb') as fid:\n",
    "      serialized_graph = fid.read()\n",
    "      od_graph_def.ParseFromString(serialized_graph)\n",
    "      tf.import_graph_def(od_graph_def, name='')\n",
    "  return od_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CoOK2n0ZLwVi"
   },
   "source": [
    "Download the pretrained object detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "zZaA3ddA4-IW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to download\n",
      "    from: 'https://storage.googleapis.com/dm-mmt-models/frozen_inference_graph.pb'\n",
      "      to: 'C:\\Users\\Mikey\\AppData\\Local\\Temp\\frozen_inference_graph.pb'\n"
     ]
    }
   ],
   "source": [
    "!wget --no-check-certificate https://storage.googleapis.com/dm-mmt-models/frozen_inference_graph.pb -P '/tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "3qocil4KiP4K"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Mikey\\\\AppData\\\\Local\\\\Tempfrozen_inference_graph.pb'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m detection_graph \u001b[38;5;241m=\u001b[39m \u001b[43mLoadInferenceGraph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtemp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfrozen_inference_graph.pb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSuccessfully loaded frozen model from \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://storage.googleapis.com/dm-mmt-models/frozen_inference_graph.pb\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36mLoadInferenceGraph\u001b[1;34m(inference_graph_path)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m od_graph\u001b[38;5;241m.\u001b[39mas_default():\n\u001b[0;32m     12\u001b[0m   od_graph_def \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mGraphDef()\n\u001b[1;32m---> 13\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minference_graph_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fid:\n\u001b[0;32m     14\u001b[0m     serialized_graph \u001b[38;5;241m=\u001b[39m fid\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     15\u001b[0m     od_graph_def\u001b[38;5;241m.\u001b[39mParseFromString(serialized_graph)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Mikey\\\\AppData\\\\Local\\\\Tempfrozen_inference_graph.pb'"
     ]
    }
   ],
   "source": [
    "detection_graph = LoadInferenceGraph(os.environ['temp'] + '\\frozen_inference_graph.pb')\n",
    "print ('Successfully loaded frozen model from {}'.format('https://storage.googleapis.com/dm-mmt-models/frozen_inference_graph.pb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_YkafsKflGkk"
   },
   "source": [
    "### Load an Example Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e0hSW3qdmHXC"
   },
   "outputs": [],
   "source": [
    "def LoadImageIntoNumpyArray(path):\n",
    "\n",
    "  with open(path, 'rb') as img_file: \n",
    "    img = mpimg.imread(img_file)\n",
    "    (im_width, im_height) = img.shape[:2]\n",
    "    return img[:,:,:3].astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SfAsIi_z5OY4"
   },
   "outputs": [],
   "source": [
    "# Download the image\n",
    "!wget --no-check-certificate https://storage.googleapis.com/dm-mmt-models/COCO_val2014_000000570107.jpeg -P '/tmp/' \n",
    "image_np = LoadImageIntoNumpyArray('/tmp/COCO_val2014_000000570107.jpeg')\n",
    "\n",
    "print('image type: %s' % str(image_np.dtype))\n",
    "print('image shape: %s' % str(image_np.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MpVO4bCPPcrt"
   },
   "source": [
    "###Preprocessing Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7r0qXvmzQDmG"
   },
   "source": [
    "Loading the object-label mappings for the dectector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B_Fa0dI14Z5W"
   },
   "outputs": [],
   "source": [
    "!wget --no-check-certificate https://storage.googleapis.com/dm-mmt-models/objatt_labelmap.txt -P '/tmp/' \n",
    "label_map_path = '/tmp/objatt_labelmap.txt'\n",
    "categories = label_map_util.create_categories_from_labelmap(label_map_path, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RrxWiy6hPlfy"
   },
   "source": [
    "Running inference on the object detector for a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O7LeGadUzgeE"
   },
   "outputs": [],
   "source": [
    "def RunInferenceSingleImage(image, graph):\n",
    "  \"\"\"Run single image through tensorflow object detection graph.\n",
    "\n",
    "  This function runs an inference graph (frozen using the functions provided\n",
    "  in this file) on a (single) provided image and returns inference results in\n",
    "  numpy arrays.\n",
    "\n",
    "  Args:\n",
    "    image: uint8 numpy array with shape (img_height, img_width, 3)\n",
    "    graph: tensorflow graph object holding loaded model.  This graph can be\n",
    "      obtained by running the LoadInferenceGraph function above.\n",
    "\n",
    "  Returns:\n",
    "    output_dict: a dictionary holding the following entries:\n",
    "      `num_detections`: an integer\n",
    "      `detection_boxes`: a numpy (float32) array of shape [N, 4]\n",
    "      `detection_classes`: a numpy (uint8) array of shape [N]\n",
    "      `detection_scores`: a numpy (float32) array of shape [N]\n",
    "      `detection_masks`: a numpy (uint8) array of shape\n",
    "         [N, image_height, image_width] with values in {0, 1}\n",
    "      `detection_keypoints`: a numpy (float32) array of shape\n",
    "         [N, num_keypoints, 2]\n",
    "  \"\"\"\n",
    "  with graph.as_default():\n",
    "    with tf.Session() as sess:\n",
    "      # Get handles to input and output tensors\n",
    "      ops = tf.get_default_graph().get_operations()\n",
    "      all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
    "      tensor_dict = {}\n",
    "      detection_fields = fields.DetectionResultFields\n",
    "      for key in [\n",
    "          v for k, v in vars(detection_fields).items()\n",
    "          if not k.startswith('__')\n",
    "      ]:\n",
    "        tensor_name = key + ':0'\n",
    "        if tensor_name in all_tensor_names:\n",
    "          tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
    "              tensor_name)\n",
    "      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "      # Run inference\n",
    "      output_dict = sess.run(tensor_dict,\n",
    "                             feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
    "\n",
    "      # all outputs are float32 numpy arrays, so convert types as appropriate\n",
    "      output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
    "      output_dict['detection_classes'] = output_dict[\n",
    "          'detection_classes'][0].astype(np.uint8)\n",
    "      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
    "      output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
    "      if 'detection_masks' in output_dict:\n",
    "        output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
    "      if 'detection_keypoints' in output_dict:\n",
    "        output_dict['detection_keypoints'] = output_dict['detection_keypoints'][\n",
    "            0]\n",
    "  return output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mdbzypy9MtLk"
   },
   "source": [
    "Pass an Image Through the Detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5e23DVGLlc6o"
   },
   "outputs": [],
   "source": [
    "# Run inference\n",
    "output_dict = RunInferenceSingleImage(image_np, detection_graph)\n",
    "output_dict['detection_features'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G-e9VRV_8NI5"
   },
   "outputs": [],
   "source": [
    "output_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yni8oUuyQYuS"
   },
   "source": [
    "Preprocessing the output of the detector to be readable by our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hyoXWQYKPQ91"
   },
   "outputs": [],
   "source": [
    "image_seq_num = 100 \n",
    "image_feat  = {}\n",
    "image_feat['height'] = image_np.shape[0]\n",
    "image_feat['width'] = image_np.shape[1]\n",
    "\n",
    "raw_feats = np.mean(np.mean(output_dict['detection_features'], axis=-2), axis=-2).squeeze() # image_feat['detection_features']\n",
    "num_detections = output_dict['num_detections']\n",
    "\n",
    "raw_scores = output_dict['detection_multiclass_scores'][:, :num_detections, ...].squeeze()\n",
    "\n",
    "# Find regions with highest class scores\n",
    "sorted_score_idxs = np.argsort(np.max(raw_scores[:, 1:], axis=-1))[::-1]\n",
    "\n",
    "# Collect features, boxes, and scores for highest scoring regions\n",
    "detection_feats = np.zeros((image_seq_num + 1, raw_feats.shape[-1]))\n",
    "detection_scores = np.zeros((image_seq_num + 1, raw_scores.shape[-1]))\n",
    "bbox_feats = np.zeros((image_seq_num + 1, 5))\n",
    "image_padding = np.ones((image_seq_num + 1,))\n",
    "padding_offset = max(image_seq_num + 1 - sorted_score_idxs.shape[0], 0)\n",
    "\n",
    "for i, index in enumerate(sorted_score_idxs[:image_seq_num]):\n",
    "  padded_index = i + padding_offset\n",
    "  detection_feats[padded_index, :] = raw_feats[index, :]\n",
    "  detection_scores[padded_index, :] = raw_scores[index, :]\n",
    "  # index 0 is 'background'\n",
    "  bbox_feats[padded_index, :4] = output_dict['detection_boxes'][index, :]\n",
    "  bbox_w = (output_dict['detection_boxes'][index, 3] -\n",
    "            output_dict['detection_boxes'][index, 1]) * image_feat['width']\n",
    "  bbox_h = (output_dict['detection_boxes'][index, 2] -\n",
    "            output_dict['detection_boxes'][index, 0]) * image_feat['height']\n",
    "  bbox_area = (bbox_w * bbox_h) / (image_feat['height'] * image_feat['width'])\n",
    "  bbox_feats[padded_index, -1] = bbox_area\n",
    "  image_padding[padded_index] = 0\n",
    "\n",
    "# Add in global image feature\n",
    "detection_feats[-1, :]= np.mean(detection_feats[padding_offset:-1, ...], axis=0).squeeze()\n",
    "bbox_feats[-1, :] = [0, 0, 1, 1, 1]\n",
    "image_padding[-1] = 0\n",
    "\n",
    "features.update(\n",
    "    {'image/bboxes': bbox_feats.astype(np.float32),\n",
    "     'image/padding_mask':  image_padding.astype(np.int32), \n",
    "     'image/detection_features': detection_feats.astype(np.float32),\n",
    "     'image/detection_scores': detection_scores.astype(np.float32)})               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dsS_lflb3lO5"
   },
   "outputs": [],
   "source": [
    "print(features['image/bboxes'].shape)\n",
    "print(features['image/detection_features'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgqt6SFBqEeH"
   },
   "source": [
    "### Visualizing the Detector Regions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5bIAo59YyOu0"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "detection_classes = []\n",
    "detection_scores = []\n",
    "tuplet_index = {}\n",
    "\n",
    "for i in range(100):\n",
    "  raw_detection_scores_obj = output_dict['detection_multiclass_scores'][:,i,1:1600][0,:]\n",
    "  raw_detection_scores_att = output_dict['detection_multiclass_scores'][:,i,1600:][0,:]\n",
    "  max_obj = np.argmax(raw_detection_scores_obj)\n",
    "  max_att = np.argmax(raw_detection_scores_att)\n",
    "  tuplet_index[i] = {}\n",
    "  tuplet_index[i]['name'] = '%s %s' %(category_index[max_att+1600]['name'],\n",
    "                              category_index[max_obj+1]['name'])\n",
    "  detection_classes.append(i)\n",
    "  detection_scores.append(raw_detection_scores_obj[max_obj] +\n",
    "                          raw_detection_scores_att[max_att])\n",
    "\n",
    "# Create detections visualization\n",
    "bboxes = vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "    image_np.copy(),\n",
    "    output_dict['detection_boxes'],\n",
    "    np.array(detection_classes),\n",
    "    detection_scores,\n",
    "    tuplet_index,\n",
    "    instance_masks=None,\n",
    "    use_normalized_coordinates=True,\n",
    "    max_boxes_to_draw=15,\n",
    "    min_score_thresh=.05,\n",
    "    agnostic_mode=False)\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "_ = plt.imshow(bboxes)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3nlv5_IyX53t"
   },
   "source": [
    "# Running Image-Text Pairs through the MMT\n",
    "\n",
    "Now that we have extracted our image and text features we can run them through our MMT model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJpau4aEYIqO"
   },
   "source": [
    "## Use features extracted in colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y5q0zjE3oY9_"
   },
   "outputs": [],
   "source": [
    "# Select a model\n",
    "\n",
    "#@title Category-conditional sampling { display-mode: \"form\", run: \"auto\" }\n",
    "\n",
    "tags = ['architecture-ft_image-q-12',\n",
    "        'architecture-ft_image-q-24',\n",
    "        'architecture-ft_language-q-12',\n",
    "        'architecture-ft_language-q-24',\n",
    "        'architecture-ft_single-modality',\n",
    "        'architecture-ft_single-stream',\n",
    "        'architecture_heads1-768',\n",
    "        'architecture_heads18-64',\n",
    "        'architecture_heads3-256',\n",
    "        'architecture_heads6-64',\n",
    "        'architecture_image-q-12',\n",
    "        'architecture_image-q-24',\n",
    "        'architecture_language-q-12',\n",
    "        'architecture_language-q-24',\n",
    "        'architecture_mixed-modality',\n",
    "        'architecture_single-modality',\n",
    "        'architecture_single-modality-hloss',\n",
    "        'architecture_single-stream',\n",
    "        'architecture_vilbert-12block',\n",
    "        'architecture_vilbert-1block',\n",
    "        'architecture_vilbert-2block',\n",
    "        'architecture_vilbert-4block',\n",
    "        'baseline-ft_baseline',\n",
    "        'baseline-ft_baseline-cls',\n",
    "        'baseline-ft_baseline-no-bert-transfer',\n",
    "        'baseline_baseline',\n",
    "        'baseline_baseline-cls',\n",
    "        'baseline_baseline-no-bert-transfer',\n",
    "        'data-ft_cc',\n",
    "        'data-ft_combined-dataset',\n",
    "        'data-ft_combined-instance',\n",
    "        'data-ft_mscoco',\n",
    "        'data-ft_mscoco-narratives',\n",
    "        'data-ft_oi-narratives',\n",
    "        'data-ft_sbu',\n",
    "        'data-ft_uniter-dataset',\n",
    "        'data-ft_uniter-instance',\n",
    "        'data-ft_vg',\n",
    "        'data_cc',\n",
    "        'data_cc-with-bert',\n",
    "        'data_combined-dataset',\n",
    "        'data_combined-instance',\n",
    "        'data_mscoco',\n",
    "        'data_mscoco-narratives',\n",
    "        'data_oi-narratives',\n",
    "        'data_sbu',\n",
    "        'data_uniter-dataset',\n",
    "        'data_uniter-instance',\n",
    "        'data_vg',\n",
    "        'loss_itm+mrm',\n",
    "        'loss_itm_mrm',\n",
    "        'loss_single-modality-contrastive1024',\n",
    "        'loss_single-modality-contrastive32',\n",
    "        'loss_v1-contrastive32',\n",
    "        'pixel_vilbert_cc-full-image']\n",
    "\n",
    "model = \"data_cc\" #@param [\"architecture-ft_image-q-12\", \"architecture-ft_image-q-24\", \"architecture-ft_language-q-12\", \"architecture-ft_language-q-24\", \"architecture-ft_single-modality\", \"architecture-ft_single-stream\", \"architecture_heads1-768\", \"architecture_heads18-64\", \"architecture_heads3-256\", \"architecture_heads6-64\", \"architecture_image-q-12\", \"architecture_image-q-24\", \"architecture_language-q-12\", \"architecture_language-q-24\", \"architecture_mixed-modality\", \"architecture_single-modality\", \"architecture_single-modality-hloss\", \"architecture_single-stream\", \"architecture_vilbert-12block\", \"architecture_vilbert-1block\", \"architecture_vilbert-2block\", \"architecture_vilbert-4block\", \"baseline-ft_baseline\", \"baseline-ft_baseline-cls\", \"baseline-ft_baseline-no-bert-transfer\", \"baseline_baseline\", \"baseline_baseline-cls\", \"baseline_baseline-no-bert-transfer\", \"data-ft_cc\", \"data-ft_combined-dataset\", \"data-ft_combined-instance\", \"data-ft_mscoco\", \"data-ft_mscoco-narratives\", \"data-ft_oi-narratives\", \"data-ft_sbu\", \"data-ft_uniter-dataset\", \"data-ft_uniter-instance\", \"data-ft_vg\", \"data_cc\", \"data_cc-with-bert\", \"data_combined-dataset\", \"data_combined-instance\", \"data_mscoco\", \"data_mscoco-narratives\", \"data_oi-narratives\", \"data_sbu\", \"data_uniter-dataset\", \"data_uniter-instance\", \"data_vg\", \"loss_itm+mrm\", \"loss_itm_mrm\", \"loss_single-modality-contrastive1024\", \"loss_single-modality-contrastive32\", \"loss_v1-contrastive32\"]\n",
    "\n",
    "tfhub_link = \"https://tfhub.dev/deepmind/mmt/%s/1\" %model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ugmFX5OYOfi"
   },
   "outputs": [],
   "source": [
    "model = hub.load(tfhub_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bhcy1XN3YvIw"
   },
   "outputs": [],
   "source": [
    "inputs={'image/bboxes': tf.expand_dims(features['image/bboxes'], 0),\n",
    "        'text/padding_mask': tf.expand_dims(features['text/padding_mask'], 0),\n",
    "        'image/padding_mask': tf.expand_dims(features['image/padding_mask'], 0),\n",
    "        'masked_tokens': tf.expand_dims(features['text/token_ids'], 0),\n",
    "        'text/segment_ids': tf.expand_dims(features['text/segment_ids'], 0),\n",
    "        'image/detection_features': tf.expand_dims(features['image/detection_features'], 0),\n",
    "        'text/token_ids': tf.expand_dims(features['text/token_ids'], 0)\n",
    "          }\n",
    "\n",
    "output = model.signatures['default'](**inputs)\n",
    "score = tf.nn.softmax(output['output']).numpy()[0]\n",
    "\n",
    "if score > 0.5:\n",
    "  print('The text and image match!  (score: %0.03f)' %score)\n",
    "else: \n",
    "  print('The text and image do not match :( (score: %0.03f)' %score) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KwlfRwMaVge6"
   },
   "source": [
    "# Running with Pre-Extracted Features\n",
    "\n",
    "We have pre-extracted MSCOCO and Flickr image features.  You can uset these pre-extracted features to do retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cEyRvb-YO8K"
   },
   "source": [
    "## Use Precomputed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "39CJERs_PJfd"
   },
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "!wget --no-check-certificate https://storage.googleapis.com/dm-mmt-models/features/coco_test/570107.pkl -P '/tmp/' \n",
    "with open('/tmp/570107.pkl', 'rb') as f:\n",
    "  im_feats = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H_41h80NYH3o"
   },
   "outputs": [],
   "source": [
    "inputs={'image/bboxes': tf.expand_dims(features['image/bboxes'], 0),\n",
    "        'text/padding_mask': tf.expand_dims(features['text/padding_mask'], 0),\n",
    "        'image/padding_mask': tf.expand_dims(im_feats['image/padding_mask'], 0),\n",
    "        'masked_tokens': tf.expand_dims(features['text/token_ids'], 0),\n",
    "        'text/segment_ids': tf.expand_dims(features['text/segment_ids'], 0),\n",
    "        'image/detection_features': tf.expand_dims(im_feats['image/detection_features'], 0),\n",
    "        'text/token_ids': tf.expand_dims(features['text/token_ids'], 0)\n",
    "          }\n",
    "\n",
    "output = model.signatures['default'](**inputs)\n",
    "score = tf.nn.softmax(output['output']).numpy()[0]\n",
    "\n",
    "if score > 0.5:\n",
    "  print('The text and image match!  (score: %0.03f)' %score)\n",
    "else: \n",
    "  print('The text and image do not match :( (score: %0.03f)' %score) "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "mmt_release.ipynb",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1y-QoDky8RQTvM6shgfYguEJs8Bg4F3jg",
     "timestamp": 1635456767570
    },
    {
     "file_id": "1HEb0vz63HwqhY7vafLTetV3r4dAZ7iNV",
     "timestamp": 1628272609748
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
